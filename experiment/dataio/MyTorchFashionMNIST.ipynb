{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "# from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "labels_dict = {\n",
        "    0: \"Camiseta/Top (T-Shirt/Top)\",\n",
        "    1: \"Pantalón (Trouser)\",\n",
        "    2: \"Jersey (Pullover)\",\n",
        "    3: \"Vestido (Dress)\",\n",
        "    4: \"Abrigo (Coat)\",\n",
        "    5: \"Sandalia (Sandal)\",\n",
        "    6: \"Camisa (Shirt)\",\n",
        "    7: \"Zapatilla (Sneaker)\",\n",
        "    8: \"Bolso (Bag)\",\n",
        "    9: \"Botín (Ankle Boot)\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St2VDfojBfZG",
        "outputId": "5ac139fe-d89b-42c6-ebb6-45a72c639243"
      },
      "outputs": [],
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLzUjRjErXT3",
        "outputId": "31db157f-0283-48e7-ac25-8592fad42426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The train set contains 60000 images, in 938 batches\n",
            "The test set contains 10000 images, in 157 batches\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"The train set contains {} images, in {} batches\".format(len(train_dataloader.dataset), len(train_dataloader)))\n",
        "# print(\"The validation set contains {} images, in {} batches\".format(len(valid_loader.dataset), len(valid_loader)))\n",
        "print(\"The test set contains {} images, in {} batches\".format(len(test_dataloader.dataset), len(test_dataloader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXsRg8cWrb0b",
        "outputId": "9cdf9872-82fc-4eb7-d048-ac1710d65b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from logging import log\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu = nn.Sequential(\n",
        "        nn.Linear(28*28, 128),\n",
        "        nn.ReLU(),       \n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "model = Net().to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# loss_fn = nn.SoftMarginLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J7Atj3fCrsBL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred,y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for X,y in dataloader:\n",
        "      X,y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sjums5-Er3Ie",
        "outputId": "6b680640-c4e2-49ac-9df8-49422403809e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302166 [    0/60000]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\usuario\\anaconda3\\envs\\houdini\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 1.874036 [ 6400/60000]\n",
            "loss: 1.756269 [12800/60000]\n",
            "loss: 1.810364 [19200/60000]\n",
            "loss: 1.751698 [25600/60000]\n",
            "loss: 1.696662 [32000/60000]\n",
            "loss: 1.697043 [38400/60000]\n",
            "loss: 1.675392 [44800/60000]\n",
            "loss: 1.615767 [51200/60000]\n",
            "loss: 1.702729 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 1.655128 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.619919 [    0/60000]\n",
            "loss: 1.685158 [ 6400/60000]\n",
            "loss: 1.582764 [12800/60000]\n",
            "loss: 1.696723 [19200/60000]\n",
            "loss: 1.678413 [25600/60000]\n",
            "loss: 1.611206 [32000/60000]\n",
            "loss: 1.636673 [38400/60000]\n",
            "loss: 1.702369 [44800/60000]\n",
            "loss: 1.662516 [51200/60000]\n",
            "loss: 1.644820 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 1.626918 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.609441 [    0/60000]\n",
            "loss: 1.648451 [ 6400/60000]\n",
            "loss: 1.547835 [12800/60000]\n",
            "loss: 1.669425 [19200/60000]\n",
            "loss: 1.654956 [25600/60000]\n",
            "loss: 1.626909 [32000/60000]\n",
            "loss: 1.588059 [38400/60000]\n",
            "loss: 1.651268 [44800/60000]\n",
            "loss: 1.635005 [51200/60000]\n",
            "loss: 1.625156 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.8%, Avg loss: 1.623808 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.619969 [    0/60000]\n",
            "loss: 1.591318 [ 6400/60000]\n",
            "loss: 1.589169 [12800/60000]\n",
            "loss: 1.623004 [19200/60000]\n",
            "loss: 1.646129 [25600/60000]\n",
            "loss: 1.648094 [32000/60000]\n",
            "loss: 1.553957 [38400/60000]\n",
            "loss: 1.666615 [44800/60000]\n",
            "loss: 1.626522 [51200/60000]\n",
            "loss: 1.631137 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 1.615926 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.573082 [    0/60000]\n",
            "loss: 1.640388 [ 6400/60000]\n",
            "loss: 1.617128 [12800/60000]\n",
            "loss: 1.669101 [19200/60000]\n",
            "loss: 1.606066 [25600/60000]\n",
            "loss: 1.638687 [32000/60000]\n",
            "loss: 1.571716 [38400/60000]\n",
            "loss: 1.652550 [44800/60000]\n",
            "loss: 1.647969 [51200/60000]\n",
            "loss: 1.624919 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.3%, Avg loss: 1.608936 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.558456 [    0/60000]\n",
            "loss: 1.594531 [ 6400/60000]\n",
            "loss: 1.551696 [12800/60000]\n",
            "loss: 1.646638 [19200/60000]\n",
            "loss: 1.592238 [25600/60000]\n",
            "loss: 1.612962 [32000/60000]\n",
            "loss: 1.616083 [38400/60000]\n",
            "loss: 1.630719 [44800/60000]\n",
            "loss: 1.673810 [51200/60000]\n",
            "loss: 1.615693 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 1.618111 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.561696 [    0/60000]\n",
            "loss: 1.582715 [ 6400/60000]\n",
            "loss: 1.566786 [12800/60000]\n",
            "loss: 1.674199 [19200/60000]\n",
            "loss: 1.582786 [25600/60000]\n",
            "loss: 1.612714 [32000/60000]\n",
            "loss: 1.600195 [38400/60000]\n",
            "loss: 1.658864 [44800/60000]\n",
            "loss: 1.646282 [51200/60000]\n",
            "loss: 1.608724 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.4%, Avg loss: 1.606956 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.562253 [    0/60000]\n",
            "loss: 1.588078 [ 6400/60000]\n",
            "loss: 1.567402 [12800/60000]\n",
            "loss: 1.599187 [19200/60000]\n",
            "loss: 1.616842 [25600/60000]\n",
            "loss: 1.617985 [32000/60000]\n",
            "loss: 1.603302 [38400/60000]\n",
            "loss: 1.649674 [44800/60000]\n",
            "loss: 1.625694 [51200/60000]\n",
            "loss: 1.646065 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 1.602744 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.565534 [    0/60000]\n",
            "loss: 1.577288 [ 6400/60000]\n",
            "loss: 1.509945 [12800/60000]\n",
            "loss: 1.632627 [19200/60000]\n",
            "loss: 1.619827 [25600/60000]\n",
            "loss: 1.654084 [32000/60000]\n",
            "loss: 1.604195 [38400/60000]\n",
            "loss: 1.603606 [44800/60000]\n",
            "loss: 1.602731 [51200/60000]\n",
            "loss: 1.597927 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 1.596984 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.566769 [    0/60000]\n",
            "loss: 1.591383 [ 6400/60000]\n",
            "loss: 1.568441 [12800/60000]\n",
            "loss: 1.639624 [19200/60000]\n",
            "loss: 1.604655 [25600/60000]\n",
            "loss: 1.618584 [32000/60000]\n",
            "loss: 1.587186 [38400/60000]\n",
            "loss: 1.617415 [44800/60000]\n",
            "loss: 1.609626 [51200/60000]\n",
            "loss: 1.617371 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 1.600146 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.564778 [    0/60000]\n",
            "loss: 1.598182 [ 6400/60000]\n",
            "loss: 1.535859 [12800/60000]\n",
            "loss: 1.592631 [19200/60000]\n",
            "loss: 1.601009 [25600/60000]\n",
            "loss: 1.610994 [32000/60000]\n",
            "loss: 1.574179 [38400/60000]\n",
            "loss: 1.629781 [44800/60000]\n",
            "loss: 1.643255 [51200/60000]\n",
            "loss: 1.643484 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.1%, Avg loss: 1.600217 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.567769 [    0/60000]\n",
            "loss: 1.569967 [ 6400/60000]\n",
            "loss: 1.586123 [12800/60000]\n",
            "loss: 1.690818 [19200/60000]\n",
            "loss: 1.613322 [25600/60000]\n",
            "loss: 1.638074 [32000/60000]\n",
            "loss: 1.570279 [38400/60000]\n",
            "loss: 1.623609 [44800/60000]\n",
            "loss: 1.633590 [51200/60000]\n",
            "loss: 1.608206 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.8%, Avg loss: 1.602438 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.576079 [    0/60000]\n",
            "loss: 1.601145 [ 6400/60000]\n",
            "loss: 1.606836 [12800/60000]\n",
            "loss: 1.656435 [19200/60000]\n",
            "loss: 1.626752 [25600/60000]\n",
            "loss: 1.629443 [32000/60000]\n",
            "loss: 1.584002 [38400/60000]\n",
            "loss: 1.635978 [44800/60000]\n",
            "loss: 1.607156 [51200/60000]\n",
            "loss: 1.612034 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 1.595035 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.560857 [    0/60000]\n",
            "loss: 1.570914 [ 6400/60000]\n",
            "loss: 1.552463 [12800/60000]\n",
            "loss: 1.582575 [19200/60000]\n",
            "loss: 1.630115 [25600/60000]\n",
            "loss: 1.576285 [32000/60000]\n",
            "loss: 1.613005 [38400/60000]\n",
            "loss: 1.652518 [44800/60000]\n",
            "loss: 1.586082 [51200/60000]\n",
            "loss: 1.589318 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 1.595048 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.588511 [    0/60000]\n",
            "loss: 1.551475 [ 6400/60000]\n",
            "loss: 1.539351 [12800/60000]\n",
            "loss: 1.634856 [19200/60000]\n",
            "loss: 1.629386 [25600/60000]\n",
            "loss: 1.610372 [32000/60000]\n",
            "loss: 1.593847 [38400/60000]\n",
            "loss: 1.619145 [44800/60000]\n",
            "loss: 1.589249 [51200/60000]\n",
            "loss: 1.605085 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 1.599844 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.556366 [    0/60000]\n",
            "loss: 1.589141 [ 6400/60000]\n",
            "loss: 1.571785 [12800/60000]\n",
            "loss: 1.594552 [19200/60000]\n",
            "loss: 1.611236 [25600/60000]\n",
            "loss: 1.637258 [32000/60000]\n",
            "loss: 1.582109 [38400/60000]\n",
            "loss: 1.645179 [44800/60000]\n",
            "loss: 1.576981 [51200/60000]\n",
            "loss: 1.606897 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 1.591955 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.526145 [    0/60000]\n",
            "loss: 1.643736 [ 6400/60000]\n",
            "loss: 1.573554 [12800/60000]\n",
            "loss: 1.609292 [19200/60000]\n",
            "loss: 1.596870 [25600/60000]\n",
            "loss: 1.619130 [32000/60000]\n",
            "loss: 1.605702 [38400/60000]\n",
            "loss: 1.595745 [44800/60000]\n",
            "loss: 1.622647 [51200/60000]\n",
            "loss: 1.586618 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 1.597168 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.524128 [    0/60000]\n",
            "loss: 1.595929 [ 6400/60000]\n",
            "loss: 1.552501 [12800/60000]\n",
            "loss: 1.635012 [19200/60000]\n",
            "loss: 1.618896 [25600/60000]\n",
            "loss: 1.597470 [32000/60000]\n",
            "loss: 1.593574 [38400/60000]\n",
            "loss: 1.635978 [44800/60000]\n",
            "loss: 1.601233 [51200/60000]\n",
            "loss: 1.597736 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 1.602616 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.563118 [    0/60000]\n",
            "loss: 1.584694 [ 6400/60000]\n",
            "loss: 1.539698 [12800/60000]\n",
            "loss: 1.652072 [19200/60000]\n",
            "loss: 1.624418 [25600/60000]\n",
            "loss: 1.654394 [32000/60000]\n",
            "loss: 1.602789 [38400/60000]\n",
            "loss: 1.611755 [44800/60000]\n",
            "loss: 1.632137 [51200/60000]\n",
            "loss: 1.645054 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 1.597510 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.571987 [    0/60000]\n",
            "loss: 1.555614 [ 6400/60000]\n",
            "loss: 1.569432 [12800/60000]\n",
            "loss: 1.619928 [19200/60000]\n",
            "loss: 1.631515 [25600/60000]\n",
            "loss: 1.639782 [32000/60000]\n",
            "loss: 1.586524 [38400/60000]\n",
            "loss: 1.610447 [44800/60000]\n",
            "loss: 1.597748 [51200/60000]\n",
            "loss: 1.650794 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 1.590254 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.539799 [    0/60000]\n",
            "loss: 1.595775 [ 6400/60000]\n",
            "loss: 1.540850 [12800/60000]\n",
            "loss: 1.602068 [19200/60000]\n",
            "loss: 1.625435 [25600/60000]\n",
            "loss: 1.620833 [32000/60000]\n",
            "loss: 1.630027 [38400/60000]\n",
            "loss: 1.603794 [44800/60000]\n",
            "loss: 1.579073 [51200/60000]\n",
            "loss: 1.564603 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 1.593857 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.583880 [    0/60000]\n",
            "loss: 1.558506 [ 6400/60000]\n",
            "loss: 1.537936 [12800/60000]\n",
            "loss: 1.594149 [19200/60000]\n",
            "loss: 1.657967 [25600/60000]\n",
            "loss: 1.570561 [32000/60000]\n",
            "loss: 1.630272 [38400/60000]\n",
            "loss: 1.656354 [44800/60000]\n",
            "loss: 1.606913 [51200/60000]\n",
            "loss: 1.636459 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 1.589850 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.521605 [    0/60000]\n",
            "loss: 1.540648 [ 6400/60000]\n",
            "loss: 1.584055 [12800/60000]\n",
            "loss: 1.634918 [19200/60000]\n",
            "loss: 1.602338 [25600/60000]\n",
            "loss: 1.644063 [32000/60000]\n",
            "loss: 1.604894 [38400/60000]\n",
            "loss: 1.637100 [44800/60000]\n",
            "loss: 1.575039 [51200/60000]\n",
            "loss: 1.578898 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 1.591362 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.559122 [    0/60000]\n",
            "loss: 1.539778 [ 6400/60000]\n",
            "loss: 1.510789 [12800/60000]\n",
            "loss: 1.600253 [19200/60000]\n",
            "loss: 1.625519 [25600/60000]\n",
            "loss: 1.625429 [32000/60000]\n",
            "loss: 1.599404 [38400/60000]\n",
            "loss: 1.608038 [44800/60000]\n",
            "loss: 1.600859 [51200/60000]\n",
            "loss: 1.585375 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 1.590086 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.583472 [    0/60000]\n",
            "loss: 1.542521 [ 6400/60000]\n",
            "loss: 1.574058 [12800/60000]\n",
            "loss: 1.614266 [19200/60000]\n",
            "loss: 1.630974 [25600/60000]\n",
            "loss: 1.563605 [32000/60000]\n",
            "loss: 1.608553 [38400/60000]\n",
            "loss: 1.620512 [44800/60000]\n",
            "loss: 1.573496 [51200/60000]\n",
            "loss: 1.612499 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 1.589337 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.547646 [    0/60000]\n",
            "loss: 1.562547 [ 6400/60000]\n",
            "loss: 1.499751 [12800/60000]\n",
            "loss: 1.611115 [19200/60000]\n",
            "loss: 1.587669 [25600/60000]\n",
            "loss: 1.599092 [32000/60000]\n",
            "loss: 1.602097 [38400/60000]\n",
            "loss: 1.617824 [44800/60000]\n",
            "loss: 1.587153 [51200/60000]\n",
            "loss: 1.570499 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 1.595433 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 1.524724 [    0/60000]\n",
            "loss: 1.571533 [ 6400/60000]\n",
            "loss: 1.543923 [12800/60000]\n",
            "loss: 1.602985 [19200/60000]\n",
            "loss: 1.611326 [25600/60000]\n",
            "loss: 1.617133 [32000/60000]\n",
            "loss: 1.586545 [38400/60000]\n",
            "loss: 1.645154 [44800/60000]\n",
            "loss: 1.572657 [51200/60000]\n",
            "loss: 1.585720 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 1.591328 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.525061 [    0/60000]\n",
            "loss: 1.567617 [ 6400/60000]\n",
            "loss: 1.532072 [12800/60000]\n",
            "loss: 1.641941 [19200/60000]\n",
            "loss: 1.599918 [25600/60000]\n",
            "loss: 1.571481 [32000/60000]\n",
            "loss: 1.603375 [38400/60000]\n",
            "loss: 1.651790 [44800/60000]\n",
            "loss: 1.559014 [51200/60000]\n",
            "loss: 1.592384 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 1.596612 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 1.541683 [    0/60000]\n",
            "loss: 1.560976 [ 6400/60000]\n",
            "loss: 1.573018 [12800/60000]\n",
            "loss: 1.631991 [19200/60000]\n",
            "loss: 1.594215 [25600/60000]\n",
            "loss: 1.588860 [32000/60000]\n",
            "loss: 1.582729 [38400/60000]\n",
            "loss: 1.633111 [44800/60000]\n",
            "loss: 1.613730 [51200/60000]\n",
            "loss: 1.553306 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 1.593990 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 1.546105 [    0/60000]\n",
            "loss: 1.582303 [ 6400/60000]\n",
            "loss: 1.525014 [12800/60000]\n",
            "loss: 1.695893 [19200/60000]\n",
            "loss: 1.570168 [25600/60000]\n",
            "loss: 1.611984 [32000/60000]\n",
            "loss: 1.603575 [38400/60000]\n",
            "loss: 1.630666 [44800/60000]\n",
            "loss: 1.595567 [51200/60000]\n",
            "loss: 1.584207 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 1.588478 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 1.523196 [    0/60000]\n",
            "loss: 1.566170 [ 6400/60000]\n",
            "loss: 1.539958 [12800/60000]\n",
            "loss: 1.638595 [19200/60000]\n",
            "loss: 1.610499 [25600/60000]\n",
            "loss: 1.570492 [32000/60000]\n",
            "loss: 1.567511 [38400/60000]\n",
            "loss: 1.591729 [44800/60000]\n",
            "loss: 1.591006 [51200/60000]\n",
            "loss: 1.570261 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 1.587712 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 1.542030 [    0/60000]\n",
            "loss: 1.578442 [ 6400/60000]\n",
            "loss: 1.541628 [12800/60000]\n",
            "loss: 1.637723 [19200/60000]\n",
            "loss: 1.616843 [25600/60000]\n",
            "loss: 1.616639 [32000/60000]\n",
            "loss: 1.619788 [38400/60000]\n",
            "loss: 1.617230 [44800/60000]\n",
            "loss: 1.555269 [51200/60000]\n",
            "loss: 1.553399 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 1.589246 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 1.573543 [    0/60000]\n",
            "loss: 1.611547 [ 6400/60000]\n",
            "loss: 1.584089 [12800/60000]\n",
            "loss: 1.579316 [19200/60000]\n",
            "loss: 1.627132 [25600/60000]\n",
            "loss: 1.568099 [32000/60000]\n",
            "loss: 1.587378 [38400/60000]\n",
            "loss: 1.622973 [44800/60000]\n",
            "loss: 1.592439 [51200/60000]\n",
            "loss: 1.632328 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 1.594529 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 1.524358 [    0/60000]\n",
            "loss: 1.628420 [ 6400/60000]\n",
            "loss: 1.538234 [12800/60000]\n",
            "loss: 1.601466 [19200/60000]\n",
            "loss: 1.624631 [25600/60000]\n",
            "loss: 1.585703 [32000/60000]\n",
            "loss: 1.611427 [38400/60000]\n",
            "loss: 1.630444 [44800/60000]\n",
            "loss: 1.571453 [51200/60000]\n",
            "loss: 1.588282 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 1.594213 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 1.538478 [    0/60000]\n",
            "loss: 1.617846 [ 6400/60000]\n",
            "loss: 1.545933 [12800/60000]\n",
            "loss: 1.623465 [19200/60000]\n",
            "loss: 1.660085 [25600/60000]\n",
            "loss: 1.563358 [32000/60000]\n",
            "loss: 1.588695 [38400/60000]\n",
            "loss: 1.668303 [44800/60000]\n",
            "loss: 1.572309 [51200/60000]\n",
            "loss: 1.577048 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 1.589150 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 1.514493 [    0/60000]\n",
            "loss: 1.559576 [ 6400/60000]\n",
            "loss: 1.562816 [12800/60000]\n",
            "loss: 1.633084 [19200/60000]\n",
            "loss: 1.600862 [25600/60000]\n",
            "loss: 1.601656 [32000/60000]\n",
            "loss: 1.587623 [38400/60000]\n",
            "loss: 1.644027 [44800/60000]\n",
            "loss: 1.573516 [51200/60000]\n",
            "loss: 1.549953 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 1.591587 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 1.555618 [    0/60000]\n",
            "loss: 1.529150 [ 6400/60000]\n",
            "loss: 1.558525 [12800/60000]\n",
            "loss: 1.609058 [19200/60000]\n",
            "loss: 1.602965 [25600/60000]\n",
            "loss: 1.584496 [32000/60000]\n",
            "loss: 1.616161 [38400/60000]\n",
            "loss: 1.622436 [44800/60000]\n",
            "loss: 1.585085 [51200/60000]\n",
            "loss: 1.534991 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 1.592301 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 1.537953 [    0/60000]\n",
            "loss: 1.587745 [ 6400/60000]\n",
            "loss: 1.554810 [12800/60000]\n",
            "loss: 1.660406 [19200/60000]\n",
            "loss: 1.585201 [25600/60000]\n",
            "loss: 1.575956 [32000/60000]\n",
            "loss: 1.575155 [38400/60000]\n",
            "loss: 1.589024 [44800/60000]\n",
            "loss: 1.618358 [51200/60000]\n",
            "loss: 1.621807 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 1.589606 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 1.533237 [    0/60000]\n",
            "loss: 1.605936 [ 6400/60000]\n",
            "loss: 1.551699 [12800/60000]\n",
            "loss: 1.642704 [19200/60000]\n",
            "loss: 1.616320 [25600/60000]\n",
            "loss: 1.599657 [32000/60000]\n",
            "loss: 1.616668 [38400/60000]\n",
            "loss: 1.651474 [44800/60000]\n",
            "loss: 1.597018 [51200/60000]\n",
            "loss: 1.552993 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 1.592156 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 1.533845 [    0/60000]\n",
            "loss: 1.565232 [ 6400/60000]\n",
            "loss: 1.537071 [12800/60000]\n",
            "loss: 1.644874 [19200/60000]\n",
            "loss: 1.617329 [25600/60000]\n",
            "loss: 1.633201 [32000/60000]\n",
            "loss: 1.590534 [38400/60000]\n",
            "loss: 1.648096 [44800/60000]\n",
            "loss: 1.570572 [51200/60000]\n",
            "loss: 1.633338 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 1.592936 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 40\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hOUccyFtZy7",
        "outputId": "69ac1bf7-7c28-4afd-ceb3-3d5eefb64185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained and saved.\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'fashion_mnist_model.pth')\n",
        "\n",
        "print('Model trained and saved.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "asW2ZcUdtqNO",
        "outputId": "43e807ca-38e8-4c28-93df-000fd05a2959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: torch.Size([1, 28, 28])\n",
            "Predicted digit: 5\n",
            "Fashion: Sandalia (Sandal)\n",
            "model:\n",
            "Net(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (7): LogSoftmax(dim=None)\n",
            "  )\n",
            ")\n",
            "dir(model):\n",
            "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'flatten', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'linear_relu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
          ]
        }
      ],
      "source": [
        "# Load the saved model\n",
        "model = Net().to(device)\n",
        "model.load_state_dict(torch.load('fashion_mnist_model.pth', weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "image_path = 'z.png'\n",
        "image = Image.open(image_path).convert('L')\n",
        "# image = ImageOps.invert(image_inv)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((28, 28))\n",
        "])\n",
        "image = transform(image).to(device)\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "\n",
        "\n",
        "# Make a prediction \n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    prediction = torch.argmax(output).item()\n",
        "\n",
        "    print(f'Predicted digit: {prediction}')\n",
        "    print(f\"Fashion: {labels_dict[prediction]}\")\n",
        "    print(\"model:\\n{}\".format(model))\n",
        "    print(\"dir(model):\\n{}\".format(dir(model)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "houdini",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
